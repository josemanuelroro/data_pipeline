#========================================================================
#This script read the .csv files from Maxmind unify and save in the datalake
#in parquet format
#------------------------------------------------------------------------
#2025/01/12 Now the parquet file are generated by the Maxmind  and not the API
#
#
#
#
#
#========================================================================



import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import col,to_date,year,month,regexp_replace,udf,filter,split,lit,pow
from pyspark.sql.types import StringType,StructType, StructField,DoubleType
import findspark
import os
import warnings
import requests
from datetime import datetime
import time
findspark.init()
print(pyspark.__version__)

warnings.filterwarnings("ignore")
MI_IP="172.21.0.1"

try:
    spark.stop()
    print("Session Stopped")
except:
    print("No session actived")

spark = SparkSession.builder \
    .appName("Spark load to silver") \
    .enableHiveSupport() \
    .config("spark.hadoop.hive.metastore.uris", "thrift://hive-metastore:9083") \
    .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000") \
    .config("spark.hadoop.fs.s3a.access.key", os.getenv("MINIO_USER")) \
    .config("spark.hadoop.fs.s3a.secret.key", os.getenv("MINIO_PASSWORD")) \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false") \
    .config("spark.hadoop.fs.s3a.threads.keepalivetime", "60") \
    .config("spark.hadoop.fs.s3a.connection.establish.timeout", "5000") \
    .config("spark.hadoop.fs.s3a.connection.timeout", "10000") \
    .config("spark.hadoop.fs.s3a.socket.timeout", "10000") \
    .config("spark.hadoop.fs.s3a.paging.maximum", "5000") \
    .config("spark.hadoop.fs.s3a.multipart.size", "104857600") \
    .config("spark.hadoop.fs.s3a.multipart.threshold", "104857600") \
    .config("spark.hadoop.fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider") \
    .config("spark.hadoop.fs.s3a.multipart.purge.age", "86400") \
    .config("spark.hadoop.fs.s3a.connection.ttl", "3600") \
    .config("spark.hadoop.fs.s3a.committer.name", "directory") \
    .config("spark.hadoop.fs.s3a.committer.staging.conflict-mode", "append") \
    .getOrCreate()
spark.sparkContext.setLogLevel("ERROR")


#########################SILVER#########################



#1 Read the .log file
df = spark.read.csv(f"/app/data/nginx_logs/serv_access.log",sep=" ")

#2 Change the dateformat
df_logs = df.withColumn("_c3",to_date(regexp_replace(col("_c3"), "\\[", ""), "dd/MMM/yyyy:HH:mm:ss"))

#3 Read the Maxmind .csv
df_network = spark.read.csv(f"/app/data/scrapper_vol/source_data/asn.csv",header=True,sep=",")
df_city_ips = spark.read.csv(f"/app/data/scrapper_vol/source_data/city_ip.csv",header=True,sep=",")
df_city = spark.read.csv(f"/app/data/scrapper_vol/source_data/city.csv",header=True,sep=",")
df_country = spark.read.csv(f"/app/data/scrapper_vol/source_data/country.csv",header=True,sep=",")


df_city = df_city.select("geoname_id","city_name","country_iso_code")
df_city_ips = df_city_ips.select("network","geoname_id")

#4 Join the city_ips df  with city df
df_loc = df_city.join(df_city_ips,df_city["geoname_id"]==df_city_ips["geoname_id"],'inner')

df_loc = df_loc.select(df_city["geoname_id"],df_city["city_name"],df_city_ips["network"],df_city["country_iso_code"])

#5 Calculate the ip ranges
# ip ranges of log ip
df_con_numero = df_logs.withColumn("octetos", split(col("_c0"), "\.")) \
    .withColumn("ip_numero", 
        (col("octetos").getItem(0).cast("long") * 16777216) + 
        (col("octetos").getItem(1).cast("long") * 65536) + 
        (col("octetos").getItem(2).cast("long") * 256) + 
        (col("octetos").getItem(3).cast("long"))
    )
#ip ranges of location
df_split = df_loc.withColumn("ip_text", split(col("network"), "/").getItem(0)) \
                 .withColumn("mask", split(col("network"), "/").getItem(1).cast("int"))

df_calc = df_split.withColumn("octets", split(col("ip_text"), "\.")) \
    .withColumn("ip_start", 
                (col("octets").getItem(0).cast("long") * 16777216) + 
                (col("octets").getItem(1).cast("long") * 65536) + 
                (col("octets").getItem(2).cast("long") * 256) + 
                (col("octets").getItem(3).cast("long")))

df_final_2 = df_calc.withColumn("num_ips", pow(lit(2), lit(32) - col("mask")).cast("long")) \
                  .withColumn("ip_end", col("ip_start") + col("num_ips") - 1)
    

df_con_numero = df_con_numero.select("_c0", "ip_numero","_c3")

condicion = ((df_con_numero["ip_numero"] >= df_final_2["ip_start"]) & (df_con_numero["ip_numero"] <= df_final_2["ip_end"]))

#6 Join location dataframe ip with log ip by ranges
df_adivino = df_con_numero.join(df_final_2,condicion,"left")


#7 Join the location dataframe(city) with country df
df_final_3 = df_country.join(df_adivino, "country_iso_code", 'inner')

#8 Generate de ips for network df
df_asn_split = df_network.withColumn("ip_text", split(col("network"), "/").getItem(0)) \
                         .withColumn("mask", split(col("network"), "/").getItem(1).cast("int"))


df_asn_calc = df_asn_split.withColumn("octets", split(col("ip_text"), "\.")) \
    .withColumn("asn_start", 
        (col("octets").getItem(0).cast("long") * 16777216) + 
        (col("octets").getItem(1).cast("long") * 65536) + 
        (col("octets").getItem(2).cast("long") * 256) + 
        (col("octets").getItem(3).cast("long"))
    )


df_asn_ready = df_asn_calc.withColumn("num_ips", pow(lit(2), lit(32) - col("mask")).cast("long")) \
                          .withColumn("asn_end", col("asn_start") + col("num_ips") - 1) \
                          .select("asn_start", "asn_end", "autonomous_system_organization") 


condicion = ((df_final_3["ip_numero"] >= df_asn_ready["asn_start"]) & (df_final_3["ip_numero"] <= df_asn_ready["asn_end"]))

#9 join the dataframes
df_final_4 = df_final_3.join(df_asn_ready,condicion,'left')
df_final_5 = df_final_4.select(col("country_name").alias("country"),\
    col("city_name").alias("city"),col("network").alias("ip"),\
        col("autonomous_system_organization").alias("isp"),col("_c3").alias("fecha"))


#10 Save in silver layer
df_final_5.coalesce(1).write.mode("append").parquet(f"s3a://silver/acceso_logs/")

#11 Truncate the log file
with open("/app/data/nginx_logs/serv_access.log", 'r+') as f:
    f.truncate(0)

    
#########################GOLD#########################



df_silver = spark.read.parquet("s3a://silver/acceso_logs/")

#12 Filter dataframe
df_gold = df_silver.filter((col("country")!="local") & (col("country")!="err"))
#13 Save in gold layer
df_gold.write.mode("append").partitionBy("fecha").parquet(f"s3a://gold/acceso_logs/")